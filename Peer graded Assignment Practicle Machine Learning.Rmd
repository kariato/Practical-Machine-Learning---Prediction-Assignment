---
title: 'Peer graded Assignment Practicle Machine Learning'
author: "Mark Davey"
date: "May 17, 2017"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Peer graded Assignment: Practicle Machine Learning 

##Background

Using devices such as *Jawbone Up, Nike FuelBand, and Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from [the website here]( http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

##Data

The training data for this project are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).

The test data are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)


The data for this project come from this [source](http://groupware.les.inf.puc-rio.br/har). If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

##Prepare Environment

Clean up old data, locate to working diretory, load the caret package and set the random seed so it is reproduceable. 

```{r init, echo=TRUE, message=FALSE, warning=FALSE}
rm(list=ls()) #Clean up work area
require("knitr") #We are knitting so lets get the package
opts_knit$set(root.dir = "~/GitHub/data")
library(caret) #load required packages
library(corrplot)
set.seed(3433) #set seed so it's reproducable
```

##Download and load the data into R

Retrieve the data from the internet if not already local and then load into Data Frames.

```{r download, echo=TRUE, message=FALSE, warning=FALSE}
#setworking directory
setwd("~/GitHub/data")
#get the data from the remote system and unpack it only if does not exist
if (!file.exists("data")) {
  dir.create("data")
}
if (!file.exists("./data/pml-training.csv")) {
  fileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileURL,destfile = "./data/pml-training.csv",method="libcurl")
}
if (!file.exists("./data/pml-testing.csv")) {
  fileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(fileURL,destfile = "./data/pml-testing.csv",method="libcurl")
}
#read 
pmltraining <- read.csv("./data/pml-training.csv")
pmltesting <- read.csv("./data/pml-testing.csv")
#summary(pmltraining) get an idea of the data
dim(pmltraining) #idea of data
```

##Clean up the data

This data was large in comparison with our previous examples. Many if the fields are factors or NA so remove all those which are not numeric. Leave "classe" since it is needed as the goal.

```{r clean up the data, echo=TRUE, message=FALSE, warning=FALSE}
nums <- sapply(pmltraining, is.numeric) #find all the numeric columns
nums <- replace(nums,length(nums),TRUE) # keep the last column it contains the problem_id/classe
pmltraining <- pmltraining[,nums]
pmltesting <- pmltesting[,nums]
```

Remove data not related to the result since the problem specifies:
*goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants*

```{r clean up the data2, echo=TRUE, message=FALSE, warning=FALSE}
nums <- grepl("belt|arm|dumbell", names(pmltraining))
nums <- replace(nums,length(nums),TRUE) 
pmltraining <- pmltraining[,nums]
pmltesting <- pmltesting[,nums]
```

A lot of the data contains NA, since we hae so much data. Rather than fix it, it seemed better to remove it.

```{r clean up the data3, echo=TRUE, message=FALSE, warning=FALSE}
nums <- sapply(pmltraining, is.na) #find all the dirty column columns
nums = colSums(is.na(pmltraining)) == 0
pmltraining <- pmltraining[,nums]
pmltesting <- pmltesting[,nums]
```

Convert the goal to numeric to be used in GLM. It has to be numeric for poisson goal.

```{r clean up the data4, echo=TRUE, message=FALSE, warning=FALSE}
classNum = pmltraining$classe
levels(classNum) = 1:length(levels(pmltraining$classe))
```


## Correlation of variable

Need to identify if the data is correlated so we may need to perform PCA. 

```{r correlation, echo=TRUE, message=FALSE, warning=FALSE}
M <- cor(pmltraining[,1:39])
corrplot(M, order ="FPC",  sig.level=0.4)
```


##Cross Validation Data Split

Need to split the data for cross validation so we can identify which method has the best match

```{r split data, echo=TRUE, message=FALSE, warning=FALSE}
inTrain = createDataPartition(pmltraining$classe, p = 3/4)[[1]]
training = pmltraining[ inTrain,]
crossValidation = pmltraining[-inTrain,]
trainingCN = as.numeric(classNum[ inTrain])
testingCN= as.numeric(classNum[ -inTrain])
dim(training)
```


##Train Multiple Models  

I tried multiple methods to find the best value. Train multiple models. Could combine if needed. Will use cross validation to select the best model.

###Logistic Regrestion

```{r fit data, echo=TRUE, message=FALSE, warning=FALSE}
fit0 <- glm(trainingCN ~ ., family="poisson", data=training[,1:39])
```

###Random Forest

```{r fit data2, echo=TRUE, message=FALSE, warning=FALSE}

fit <- train(classe ~ .,
             data=training,
             method="rf",
             preProcess=c("center","scale"),
             ntree = 200) 
```

###Gradient Boosting Method

```{r fit data3, echo=TRUE, message=FALSE, warning=FALSE}
garbage <- capture.output(fit2 <- train(classe ~ .,
             data=training,method ="gbm",
               preProc = c("center","scale"),
               trControl = trainControl(preProcOptions = list(thresh = 0.8))))
```

###Naive Biase 

```{r fit data4, echo=TRUE, message=FALSE, warning=FALSE}
fit3 <- train(classe ~ .,
             data=training,method ="nb",
               preProc = c("center","scale"),
               trControl = trainControl(preProcOptions = list(thresh = 0.8)))
```

###Neural network

```{r fit data5, echo=TRUE, message=FALSE, warning=FALSE}
#fit4 <- train(classe ~ .,data=training,method="nnet", preproc = c("center","scale"),maxit = 1000)
```
The neural net refused to converge so removed it.


I initially used PCA as a pre processor but it actually hurt the response. 95% vs 99% for Random forest. This corresponds to the corrplot which showed little correlation.

##Perform machine learning

Throw all the models now find which has the best accuracy. 

```{r validate, echo=TRUE, message=FALSE, warning=FALSE}

cm1 <- confusionMatrix(crossValidation$classe,predict(fit,crossValidation))
cm1$overall[1]
cm2 <- confusionMatrix(crossValidation$classe,predict(fit2,crossValidation))
cm2$overall[1]
cm3 <- confusionMatrix(crossValidation$classe,predict(fit3,crossValidation))
cm3$overall[1]
#cm4 <- confusionMatrix(crossValidation$classe,predict(fit4,crossValidation))
#cm4$overall[1]

cm1
```

The GLM results were unusable so I've not even tried to view. 
The Random Forest result has a very close match. GBM was a close second which is strange since it's a boosted random forest. Naive Baise performed badly. I was disappointed that no neural networks are included but I could net get it to work.

##Get the result

```{r answer, echo=FALSE, message=FALSE, warning=FALSE}
answer <-  cbind(problem_id=pmltesting$problem_id,answer1=predict(fit,newdata = pmltesting),answer2=predict(fit2,newdata = pmltesting),answer3=predict(fit3,newdata = pmltesting))
answer
```

I submitted the data to the quiz and got 95% which matched by Cross Validation error which was in excees of 99%.

##Executive Summary
The data was very large and dirty requiring a lot of cleaning. Since there was so much data removing 75% of the column data was not an issue.

It was interesting how the data worked best on Random Forest vs all the other methods. Cross validation was critical to find the correct model to use. Given the large amount of data, I felt comfortable using 25% of the row data for non training purposes. 